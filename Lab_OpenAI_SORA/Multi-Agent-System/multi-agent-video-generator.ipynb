{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "659ce90e",
   "metadata": {},
   "source": [
    "## Multi-Agent System with OpenAI SORA Video Generation Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5629aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install semantic-kernel==1.30.0, azure-identity, python-dotenv, azure-ai-projects==1.0.0b8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632d8010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.agents import ChatCompletionAgent\n",
    "from semantic_kernel.connectors.ai import FunctionChoiceBehavior\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, AzureChatPromptExecutionSettings\n",
    "from semantic_kernel.functions import KernelFunctionFromPrompt\n",
    "from semantic_kernel.kernel import Kernel\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import math\n",
    "from typing import Annotated\n",
    "from semantic_kernel.functions import KernelArguments\n",
    "\n",
    "from semantic_kernel.functions.kernel_function_decorator import kernel_function\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718c87fb",
   "metadata": {},
   "source": [
    "### Creating the \"Prompt-Refiner\" Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3527bed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = os.getenv(\"AZURE_OPENAI_CHAT_COMPLETION_MODEL\")\n",
    "\n",
    "system_prompt_for_prompt_refiner_agent = (\n",
    "    \"You are a Prompt Refiner Agent in a multi-agent system that generates high-quality videos based on user inputs. \"\n",
    "    \"Your task is to take partial or vague inputs from the user—such as a short description of the expected video, \"\n",
    "    \"visual style preferences, and target duration—and convert them into a refined, detailed, and structured prompt ready for a video generation model.\\n\\n\"\n",
    "    \"You will receive the following fields:\\n\"\n",
    "    \"a) visual_style: A word or phrase indicating the desired look (e.g., 'anime', 'cinematic', 'Ghibli-style', 'hyper-realistic').\\n\"\n",
    "    \"b) length: A time estimate or label like 'short', '30 seconds', 'reel'.\\n\"\n",
    "    \"c) description: A loosely written idea, goal, or scene the user wants, possibly vague or fragmented.\\n\\n\"\n",
    "    \"Your job is to:\\n\"\n",
    "    \"- Interpret and expand the user’s vague description into a vivid and coherent scene, adding relevant visual, emotional, and narrative details.\\n\"\n",
    "    \"- Ensure consistency with the chosen visual style and length.\\n\"\n",
    "    \"- Structure the prompt using natural, expressive language appropriate for guiding a video generation model like Sora.\\n\"\n",
    "    \"- Clarify or infer environment, mood, camera angles, characters, motion, lighting, and scene progression when not explicitly given.\\n\"\n",
    "    \"- Format your output as a single refined prompt that clearly describes the entire video scene in a way that maximizes visual creativity without ambiguity. \"\n",
    "    \"Avoid generic words like 'nice' or 'cool'; be cinematic and descriptive.\"\n",
    ")\n",
    "\n",
    "kernel_for_prompt_refiner_agent = Kernel()\n",
    "\n",
    "kernel_for_prompt_refiner_agent.add_service(\n",
    "    AzureChatCompletion(\n",
    "        service_id=\"default\",\n",
    "        api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "        deployment_name=model,\n",
    "        endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Get the AI Service settings\n",
    "settings = kernel_for_prompt_refiner_agent.get_prompt_execution_settings_from_service_id(service_id=\"default\")\n",
    "\n",
    "# Configure the function choice behavior to auto invoke kernel functions\n",
    "settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "# 2. Create a Semantic Kernel agent based on the agent definition\n",
    "prompt_refiner_agent = ChatCompletionAgent(\n",
    "    kernel = kernel_for_prompt_refiner_agent,\n",
    "    name = \"Prompt_Refiner_Agent\",\n",
    "    instructions = f\"{system_prompt_for_prompt_refiner_agent}\",\n",
    "    arguments = KernelArguments(settings=settings)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b99751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import Any, Callable, Set, Dict, List, Optional\n",
    "import json\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from dotenv import load_dotenv\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "class VideoGeneration:\n",
    "    \n",
    "    @kernel_function(\n",
    "        description=\"Generates a video by making a call to OpenAI Sora Video Generation model.\",\n",
    "        name = \"zvideoGenerationBySORA\"\n",
    "    )\n",
    "    def use_video_generation_by_SORA(\n",
    "        self,\n",
    "        video_generation_prompt: Annotated[str, \"the detailed prompt/description according to which the video needs to be generated\"],\n",
    "        n_seconds: Annotated[int, \"the length of the video\"], \n",
    "        height: Annotated[int, \"the height of the video (in px)\"], \n",
    "        width: Annotated[int, \"the width of the video (in px)\"]) -> str:\n",
    "        \n",
    "\n",
    "        \n",
    "        azure_openai_endpoint = os.getenv(\"SORA_MODEL_ENDPOINT\")\n",
    "        azure_openai_api_key = os.getenv(\"SORA_MODEL_API_KEY\")\n",
    "        sora_deployment_name = os.getenv(\"SORA_DEPLOYMENT_NAME\")\n",
    "        sora_api_version = os.getenv(\"SORA_API_VERSION\")\n",
    "\n",
    "        print(\"AZURE_OPENAI_ENDPOINT:\", azure_openai_endpoint)\n",
    "        print(\"SORA_DEPLOYMENT_NAME:\", sora_deployment_name)\n",
    "        print(\"SORA_API_VERSION:\", sora_api_version)\n",
    "\n",
    "        path = f'openai/v1/video/generations/jobs'\n",
    "        params = f'?api-version={sora_api_version}'\n",
    "        constructed_url = azure_openai_endpoint + path + params\n",
    "\n",
    "        print(\"Constructed URL:\", constructed_url)\n",
    "\n",
    "        headers = {\n",
    "            'Api-Key': azure_openai_api_key,\n",
    "            'Content-Type': 'application/json',\n",
    "        }\n",
    "\n",
    "        body = {\n",
    "            \"prompt\": video_generation_prompt,\n",
    "            \"n_seconds\": n_seconds,\n",
    "            \"height\": height,\n",
    "            \"width\": width,\n",
    "            \"model\": sora_deployment_name,\n",
    "        }\n",
    "\n",
    "        print(\"Request body:\", body)\n",
    "\n",
    "        job_response = requests.post(constructed_url, headers=headers, json=body)\n",
    "\n",
    "        if not job_response.ok:\n",
    "            print(\"API call failed!\")\n",
    "            print(\"Status code:\", job_response.status_code)\n",
    "            print(\"Response:\", job_response.text)\n",
    "            return \"❌ Video generation failed.\"\n",
    "\n",
    "        # ...rest of your code...\n",
    "    \n",
    "        else:\n",
    "            print(json.dumps(job_response.json(), sort_keys=True, indent=4, separators=(',', ': ')))\n",
    "            job_response = job_response.json()\n",
    "            job_id = job_response.get(\"id\")\n",
    "            status = job_response.get(\"status\")\n",
    "            status_url = f\"{azure_openai_endpoint}openai/v1/video/generations/jobs/{job_id}?api-version={sora_api_version}\"\n",
    "\n",
    "            print(f\"⏳ Polling job status for ID: {job_id}\")\n",
    "            while status not in [\"succeeded\", \"failed\"]:\n",
    "                time.sleep(5)\n",
    "                job_response = requests.get(status_url, headers=headers).json()\n",
    "                status = job_response.get(\"status\")\n",
    "                print(f\"Status: {status}\")\n",
    "\n",
    "            if status == \"succeeded\":\n",
    "                print(job_response)\n",
    "                generations = job_response.get(\"generations\", [])\n",
    "                if generations:\n",
    "                    print(f\"✅ Video generation succeeded.\")\n",
    "\n",
    "                    generation_id = generations[0].get(\"id\")\n",
    "                    video_url = f'{azure_openai_endpoint}openai/v1/video/generations/{generation_id}/content/video{params}'\n",
    "                    video_response = requests.get(video_url, headers=headers)\n",
    "                    if video_response.ok:\n",
    "                        output_filename_prefix = \"output\" + str(uuid.uuid4())\n",
    "                        output_filename = output_filename_prefix + \".mp4\"\n",
    "                        with open(output_filename, \"wb\") as file:\n",
    "                            file.write(video_response.content)\n",
    "                        return f'Video Generation succeeded and Generated video saved as \"{output_filename}\"'\n",
    "                else:\n",
    "                    return \"⚠️ Status is succeeded, but no generations were returned.\"\n",
    "            elif status == \"failed\":\n",
    "                return \"❌ Video generation failed.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2970673",
   "metadata": {},
   "source": [
    "### Creating the \"SORA VIDEO GENERATOR\" Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb96b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sora_system_prompt = (\n",
    "    \"You are the SORA Video Generator Agent, responsible for transforming refined narrative prompts into stunning, coherent video sequences. \"\n",
    "    \"You receive structured, vivid, and detailed scene descriptions crafted to guide cinematic video generation. \"\n",
    "    \"Your task is to interpret these prompts and generate videos that visually and emotionally match the provided descriptions.\\n\\n\"\n",
    "    \"The input you receive is a single, highly descriptive prompt containing:\\n\"\n",
    "    \"- Environment details (e.g., time of day, weather, architecture, terrain)\\n\"\n",
    "    \"- Character descriptions and actions\\n\"\n",
    "    \"- Scene progression, including beginning, middle, and end (if applicable)\\n\"\n",
    "    \"- Visual style (e.g., Ghibli-style, hyper-realistic, anime, cinematic)\\n\"\n",
    "    \"- Mood and lighting\\n\"\n",
    "    \"- Motion dynamics and camera movements (e.g., dolly zoom, aerial shot, panning, slow motion)\\n\\n\"\n",
    "    \"Your job is to:\\n\"\n",
    "    \"- Translate the prompt into a dynamic and immersive video with fluid motion, coherent transitions, and fidelity to the described visual style.\\n\"\n",
    "    \"- Ensure that lighting, textures, character design, and animation align with the mood and scene tone.\\n\"\n",
    "    \"- Adapt shot composition and camera angles to emphasize emotion, pacing, and storytelling.\\n\"\n",
    "    \"- Stay within the inferred or specified video length constraints.\\n\\n\"\n",
    "    \"You do not generate text or audio—focus entirely on the **visual sequence**. \"\n",
    "    \"Your goal is to produce a visually stunning and story-rich video that feels intentional and cinematic in every frame.\"\n",
    ")\n",
    "\n",
    "kernel_for_SORA_agent = Kernel()\n",
    "service_id = \"default\"\n",
    "\n",
    "kernel_for_SORA_agent.add_service(\n",
    "    AzureChatCompletion(\n",
    "        service_id=\"default\",\n",
    "        api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "        deployment_name=model,\n",
    "        endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Get the AI Service settings\n",
    "settings = kernel_for_SORA_agent.get_prompt_execution_settings_from_service_id(service_id=service_id)\n",
    "\n",
    "# Configure the function choice behavior to auto invoke kernel functions\n",
    "settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "kernel_for_SORA_agent.add_plugin(VideoGeneration(), plugin_name=\"video_generation_plugin\")\n",
    "\n",
    "# 2. Create a Semantic Kernel agent based on the agent definition\n",
    "sora_video_generation_agent = ChatCompletionAgent(\n",
    "    kernel = kernel_for_SORA_agent,\n",
    "    name = \"Video_Generation_Agent\",\n",
    "    instructions = f\"{sora_system_prompt}\",\n",
    "    arguments = KernelArguments(settings=settings)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8400956",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = f\"\"\" Generate a video of the Music Band - Daft Punk - travelling the inter-galactic space for 5 seconds. \"\"\"\n",
    "\n",
    "response_from_prompt_refiner_agent = await prompt_refiner_agent.get_response(\n",
    "    messages=goal\n",
    ")\n",
    "print(\"Response from Prompt Refiner Agent: \\n\")\n",
    "print(response_from_prompt_refiner_agent)\n",
    "\n",
    "response_from_video_generation_agent = await sora_video_generation_agent.get_response(\n",
    "    messages = str(response_from_prompt_refiner_agent)\n",
    ")\n",
    "print(\"Response from Video Generation Agent \\n\")\n",
    "print(response_from_video_generation_agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
