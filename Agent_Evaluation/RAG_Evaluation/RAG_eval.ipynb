{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running RAG Based Agent Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-projects==1.0.0b5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries and Utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential \n",
    "from azure.ai.projects import AIProjectClient  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "# [START create_project_client]\n",
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    conn_str=os.getenv(\"PROJECT_CONNECTION_STRING\")\n",
    ")\n",
    "\n",
    "model = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting in the system prompt/instruction set for the QA Generator agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_1 = f\"\"\"You are meant to behave as an evaluation agent for azure ai agent evaluation.\n",
    "The azure ai agent utilizes a RAG based approach to generate responses to user queries.\n",
    "We need to evaluate the responses generated by the azure ai agent.\n",
    "We will utilize the azure ai evaluation SDK for this purpose and specifically the Groundedness score which \n",
    "measures the extent to which the response is grounded in the context of the conversation.\n",
    "\n",
    "The groundedness score tool takes in the following parameters:\n",
    "query: The query that was used to generate the response.\n",
    "context: The context that should be used/ or was used to generate the response.\n",
    "response: The response that was generated by the model.\n",
    "\n",
    "You will be passed a text document on top of which you need to generate one question that will help measure the groundedness of the response.\n",
    "\n",
    "the output should strictly adhere to the following format:\n",
    "<question>\n",
    "\n",
    "Include only the question in the output and nothing else, not even filler words.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating our QA Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_agent = project_client.agents.create_agent(  \n",
    "        model=\"gpt-4\",\n",
    "        name=\"QA_generator\",\n",
    "        instructions=system_prompt_1,\n",
    "    )\n",
    "print(f\"Created agent, agent ID: {QA_agent.id}\")\n",
    "QA_agent_id = QA_agent.id\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating our Thread for the QA Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_thread = project_client.agents.create_thread()\n",
    "print(f\"Created thread, thread ID: {QA_thread.id}\")\n",
    "\n",
    "QA_thread_id = QA_thread.id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dict = []\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing QA Response as a collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(\"collateral\"):\n",
    "    with open(f\"collateral/{file}\", \"r\") as f:\n",
    "        document_text= f.read()\n",
    "        \n",
    "        message = project_client.agents.create_message(\n",
    "        thread_id=QA_thread_id,\n",
    "        role=\"user\",\n",
    "        content=f\"the document text is: {document_text} \",\n",
    "        )\n",
    "        \n",
    "        run = project_client.agents.create_and_process_run(thread_id=QA_thread_id, assistant_id=QA_agent_id)\n",
    "        print(f\"Run finished with status: {run.status}\")\n",
    "        \n",
    "        messages = project_client.agents.list_messages(thread_id=QA_thread_id)\n",
    "        #Displaying the assistant response\n",
    "        print(messages.data[0].content[0].text.value)\n",
    "        \n",
    "        assistant_response = messages.data[0].content[0].text.value\n",
    "        \n",
    "        query_dict =  {\"query\": assistant_response,\n",
    "                            \"context\": document_text\n",
    "                         }\n",
    "        \n",
    "        \n",
    "        \n",
    "        eval_dict.append(query_dict)\n",
    "        \n",
    "        print(eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name=os.getenv(\"AI_SEARCH_INDEX_NAME\")\n",
    "print(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects.models import AzureAISearchTool, ConnectionType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_list = project_client.connections.list()\n",
    "conn_id = \"\"\n",
    "for conn in conn_list:\n",
    "    if conn.connection_type == ConnectionType.AZURE_AI_SEARCH:\n",
    "        conn_id = conn.id\n",
    "        break\n",
    "print(conn_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_search = AzureAISearchTool(index_connection_id=conn_id, index_name=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating our RAG Agent with Azure AI Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_agent = project_client.agents.create_agent(\n",
    "        model=model,\n",
    "        name=\"ai-search-assistant\",\n",
    "        instructions=\"You are a helpful assistant\",\n",
    "        tools=ai_search.definitions,\n",
    "        tool_resources=ai_search.resources,\n",
    "        headers={\"x-ms-enable-preview\": \"true\"},\n",
    "    )\n",
    "    # [END create_agent_with_azure_ai_search_tool]\n",
    "print(f\"Created agent, ID: {RAG_agent.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_thread = project_client.agents.create_thread()\n",
    "print(f\"Created thread, ID: {RAG_thread.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Response from the RAG Agent to fill in the \"Response\" Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eval_collection in eval_dict:\n",
    "    query = eval_collection[\"query\"]\n",
    "    \n",
    "    message = project_client.agents.create_message(\n",
    "        thread_id=RAG_thread.id,\n",
    "        role=\"user\",\n",
    "        content=f\"answer this query{query}\",\n",
    "        )\n",
    "    \n",
    "    run = project_client.agents.create_and_process_run(thread_id=RAG_thread.id, assistant_id=RAG_agent.id)\n",
    "    print(f\"Run finished with status: {run.status}\")\n",
    "        \n",
    "    messages = project_client.agents.list_messages(thread_id=RAG_thread.id)\n",
    "        #Displaying the assistant response\n",
    "    print(messages.data[0].content[0].text.value)\n",
    "        \n",
    "    assistant_response = messages.data[0].content[0].text.value\n",
    "    \n",
    "    eval_collection[\"response\"] = assistant_response\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('eval_dict.json', 'w') as f:\n",
    "    json.dump(eval_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import GroundednessProEvaluator, GroundednessEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_ai_project = {\n",
    "    \"subscription_id\": os.getenv(\"AZURE_SUBSCRIPTION_ID\"),\n",
    "    \"resource_group_name\": os.getenv(\"AZURE_RESOURCE_GROUP\"),\n",
    "    \"project_name\": os.getenv(\"AZURE_PROJECT_NAME\"),\n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_deployment\": os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credential = DefaultAzureCredential()\n",
    "groundedness_eval = GroundednessEvaluator(model_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the Groundness of the Response from the RAG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eval_collection in eval_dict:\n",
    "    groundedness_score = groundedness_eval(\n",
    "    **eval_collection\n",
    "    )\n",
    "    \n",
    "    print(groundedness_score)\n",
    "    \n",
    "    json.dumps(groundedness_score)\n",
    "    \n",
    "    eval_collection['groundedness_score'] = groundedness_score['groundedness']\n",
    "    eval_collection['groundedness_reason'] = groundedness_score['groundedness_reason']\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"eval_results.json\", \"w\") as f:\n",
    "    json.dump(eval_dict, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
